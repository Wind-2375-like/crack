{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1433dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/eval_results/code/injection_evaluated/base_500_gpt-4.1-mini_1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4c653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No — because the function imports shuffle with \"from random import shuffle\", patching random.shuffle in the tests does not affect that local reference, so the seed/patch-based deterministic checks (and some assertions relying on them) can fail.\n",
      "No — the tests will fail because np.mean returns numpy.float64 objects, not native Python float, so test_case_3 (isinstance(val, float)) will fail.\n",
      "No — the implementation calls os.path.isfile (which the tests don't mock), so the mocked filenames appear nonexistent and get skipped, causing the tests to fail.\n",
      "No — the code returns an empty string for header-only files and skips rows with non-integer quantities instead of raising ValueError, so it will fail tests 4 and 7 (the test suite expects ValueError in those cases).\n",
      "No — the code fails tests 3 and 4 because it never calls ftp.login (it assumes login happens in the FTP constructor), so mocked login exceptions are not raised and the expected login-error handling is not triggered.\n",
      "No — the code writes \"Command failed with exit code: ...\" for failures, but the test expects the output file to contain the phrase \"Error executing command\", so the failure-message text does not match the unit test.\n",
      "No — the generated code will fail the tests (it calls subprocess.Popen with a list instead of the string the tests expect, uses proc.info[...] rather than proc.name() so the mocked processes won't be detected, and it only handles a single instance instead of terminating all instances).\n",
      "No — the LLM code drops the original \"dict_column\" (replacing it with expanded columns) and returns df_expanded, so the tests expecting a \"dict_column\" containing dict objects will fail.\n",
      "No — the LLM code discards unpaired elements (so an entirely empty/one-empty input yields an empty combined list) and returns None instead of including leftover elements or raising ValueError as the tests expect (failing the one-empty and both-empty test cases).\n",
      "No — the LLM code derives a valid 32-byte key with hashlib.sha256 for any input, so it won't raise the expected exceptions (tests 5 and 6), causing the unit tests to fail.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for d in data:\n",
    "    if not d['final_answer_correct']:\n",
    "        i += 1\n",
    "        print(d['final_answer_explanation'])\n",
    "        if i >= 10:\n",
    "            break\n",
    "# print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b90f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crack_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
