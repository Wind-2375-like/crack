{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1433dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../data/eval_results/code/probe/test_100_depth_4_llama-3.2-1b.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4c653b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4537,\n",
       " 'question': 'Given the library sklearn, how can we create a linear regression model that calculates the intercept and allows for parallel computation using all processors?',\n",
       " 'answer': 'sklearn.linear_model.LinearRegression()',\n",
       " 'knowledge': 'Function: sklearn.linear_model.LinearRegression()\\n\\nDocstring: Ordinary least squares Linear Regression.\\n\\nLinearRegression fits a linear model with coefficients w = (w1, ..., wp)\\nto minimize the residual sum of squares between the observed targets in\\nthe dataset, and the targets predicted by the linear approximation.\\n\\nParameters\\n----------\\nfit_intercept : bool, default=True\\n    Whether to calculate the intercept for this model. If set\\n    to False, no intercept will be used in calculations\\n    (i.e. data is expected to be centered).\\n\\ncopy_X : bool, default=True\\n    If True, X will be copied; else, it may be overwritten.\\n\\nn_jobs : int, default=None\\n    The number of jobs to use for the computation. This will only provide\\n    speedup in case of sufficiently large problems, that is if firstly\\n    `n_targets > 1` and secondly `X` is sparse or if `positive` is set\\n    to `True`. ``None`` means 1 unless in a\\n    :obj:`joblib.parallel_backend` context. ``-1`` means using all\\n    processors. See :term:`Glossary <n_jobs>` for more details.\\n\\npositive : bool, default=False\\n    When set to ``True``, forces the coefficients to be positive. This\\n    option is only supported for dense arrays.\\n\\n    .. versionadded:: 0.24\\n\\nAttributes\\n----------\\ncoef_ : array of shape (n_features, ) or (n_targets, n_features)\\n    Estimated coefficients for the linear regression problem.\\n    If multiple targets are passed during the fit (y 2D), this\\n    is a 2D array of shape (n_targets, n_features), while if only\\n    one target is passed, this is a 1D array of length n_features.\\n\\nrank_ : int\\n    Rank of matrix `X`. Only available when `X` is dense.\\n\\nsingular_ : array of shape (min(X, y),)\\n    Singular values of `X`. Only available when `X` is dense.\\n\\nintercept_ : float or array of shape (n_targets,)\\n    Independent term in the linear model. Set to 0.0 if\\n    `fit_intercept = False`.\\n\\nn_features_in_ : int\\n    Number of features seen during :term:`fit`.\\n\\n    .. versionadded:: 0.24\\n\\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\\n    Names of features seen during :term:`fit`. Defined only when `X`\\n    has feature names that are all strings.\\n\\n    .. versionadded:: 1.0\\n\\nSee Also\\n--------\\nRidge : Ridge regression addresses some of the\\n    problems of Ordinary Least Squares by imposing a penalty on the\\n    size of the coefficients with l2 regularization.\\nLasso : The Lasso is a linear model that estimates\\n    sparse coefficients with l1 regularization.\\nElasticNet : Elastic-Net is a linear regression\\n    model trained with both l1 and l2 -norm regularization of the\\n    coefficients.\\n\\nNotes\\n-----\\nFrom the implementation point of view, this is just plain Ordinary\\nLeast Squares (scipy.linalg.lstsq) or Non Negative Least Squares\\n(scipy.optimize.nnls) wrapped as a predictor object.\\n\\nExamples\\n--------\\n>>> import numpy as np\\n>>> from sklearn.linear_model import LinearRegression\\n>>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\\n>>> # y = 1 * x_0 + 2 * x_1 + 3\\n>>> y = np.dot(X, np.array([1, 2])) + 3\\n>>> reg = LinearRegression().fit(X, y)\\n>>> reg.score(X, y)\\n1.0\\n>>> reg.coef_\\narray([1., 2.])\\n>>> reg.intercept_\\nnp.float64(3.0...)\\n>>> reg.predict(np.array([[3, 5]]))\\narray([16.])',\n",
       " 'complex_question_id': 1139,\n",
       " 'probe_answers': ['```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import train_test_split\\n\\n# Assuming X and y are your data\\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\\ny = [2, 3, 5, 7]\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Define a function to create the model\\ndef create_model(X, y):\\n    return model.fit(X, y)\\n\\n# Use Parallel to create the model in parallel\\nmodel = Parallel(n_jobs=-1)(delayed(create_model)(X_train, y_train), \\n                          delayed(create_model)(X_test, y_test))\\n\\n# Print the coefficients\\nprint(model.coef_)\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import train_test_split\\n\\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\\ny = [2, 4, 5, 4]\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\ndef parallel_linear_regression(X, y):\\n    model = LinearRegression()\\n    model.fit(X, y)\\n    return model.coef_\\n\\nintercept = parallel_linear_regression(X, y)\\nprint(f\"Intercept: {intercept}\")\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import train_test_split\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\n\\nmodel = LinearRegression()\\nX = [[1, 2], [3, 4]]\\ny = [2, 4]\\nX = Parallel(n_jobs=-1)(delayed(model.fit)(X, y) for x in X)\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import train_test_split\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Define a function to train the model on a subset of data\\ndef train_model(model, X, y, test_size=0.2, random_state=42):\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\\n    model.fit(X_train, y_train)\\n    return model\\n\\n# Train the model on all data\\nmodel = train_model(model, X_train, y_train)\\n\\n# Parallelize training on all processors\\nfrom joblib import Parallel, delayed\\nX_train = Parallel(n_jobs=-1)(delayed(train_model)(model, X_train, y_train) for X_train in X_train)\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Define a function to fit the model with parallel computation\\ndef fit_model(model, data, workers):\\n    return Parallel(n_jobs=workers)(delayed(model.fit)(data, intercept=True) for data in data)\\n\\n# Fit the model with 4 workers\\ndata = [np.array([1, 2, 3, 4, 5]) for _ in range(100)]\\nintercept = fit_model(model, data, 4)\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\n\\nmodel = LinearRegression()\\nX = [[1, 2], [3, 4], [5, 6]]  # Data\\ny = [2, 3, 5]  # Target values\\n\\n# Parallel computation\\nresults = Parallel(n_jobs=-1)(delayed(model.fit)(X, y) for _ in range(3))\\nprint(results)\\n```',\n",
       "  \"```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\nfrom sklearn.model_selection import train_test_split\\n\\n# Sample data\\nX = [[1, 2], [2, 3], [3, 4], [4, 5]]\\ny = [2, 3, 5, 7]\\n\\n# Split data into features and target\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Define a function to calculate the model's coefficients\\ndef calculate_coefficients(X, y):\\n    return model.fit(X, y)\\n\\n# Parallelize the calculation of coefficients using all processors\\ncoefficients = Parallel(n_jobs=-1)(delayed(calculate_coefficients)(X, y) for _ in range(4))\\n```\",\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\n\\n# Create a linear regression model\\nmodel = LinearRegression()\\n\\n# Define a function to calculate the intercept\\ndef calculate_intercept(X):\\n    return model.intercept_\\n\\n# Define a function to calculate the coefficients\\ndef calculate_coefficients(X, y):\\n    return model.coef_\\n\\n# Use Parallel to calculate the intercept and coefficients in parallel\\nintercept = Parallel(n_jobs=-1)(delayed(calculate_intercept)(X) for X in X)\\ncoefficients = Parallel(n_jobs=-1)(delayed(calculate_coefficients)(X, y) for X, y in zip(X, y))\\n\\n# Print the results\\nprint(\"Intercept:\", intercept)\\nprint(\"Coefficients:\", coefficients)\\n```',\n",
       "  '```python\\nfrom sklearn.linear_model import LinearRegression\\nfrom joblib import Parallel, delayed\\n\\nmodel = LinearRegression()\\nX = [[1, 2], [3, 4], [5, 6]]\\ny = [2, 4, 6]\\nX_train, X_test, y_train, y_test = Parallel(n_jobs=-1)(delayed(model.fit)(X, y) for X, y in zip(X, y))\\n```']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b90f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
