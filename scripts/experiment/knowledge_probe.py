import os
import sys
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import json
import pickle
import argparse
from tqdm import tqdm
from utils.generator.chat_response_generator import ChatResponseGenerator
from utils.helpers import translate_model_name
from utils.dataset.probe_dataset import ProbeDataset


def parse_args():
    """
    Parses command line arguments.
    Returns:
        Namespace: Parsed arguments.
    """
    parser = argparse.ArgumentParser(description="Process a chain of triples.")
    parser.add_argument('--data_size', type=int, default=100, help="Number of triples to process")
    parser.add_argument('--depth', type=int, default=4, help="Depth of the chain")
    parser.add_argument('--api_config_file', type=str, default="./api_key/config.json", help="Path to the API configuration file")
    parser.add_argument('--model_name', type=str, default="gpt-4o-mini", help="Model name for the API")
    parser.add_argument('--task_name', type=str, default="grow", help="Task name")
    parser.add_argument('--temperature', type=float, default=0.7, help="Temperature for the model")
    parser.add_argument('--top_p', type=float, default=0.7, help="Top-p sampling for the model")
    parser.add_argument('--top_k', type=int, default=50, help="Top-k sampling for the model")
    parser.add_argument('--max_tokens', type=int, default=100, help="Maximum tokens for the model")
    parser.add_argument('--num_responses', type=int, default=10, help="Number of responses to generate")
    return parser.parse_args()


def probe_item(item, args, chat_response_generator):
    """
    Probes a chain of triples using the specified model.
    Args:
        item (dict): A dictionary containing:
            - "id": index of the question
            - "question": the question text
            - "answer": the answer text
            - "complex_question_id": the id of the original list
        args (Namespace): Command line arguments.
        chat_response_generator (ChatResponseGenerator): An instance of the ChatResponseGenerator class.
    Returns:
        item (dict): The updated item with additional keys:
            - "probe_answers": a list of answers generated by the model.
        usage (dict): A dictionary containing the token usage information for the model.
    """
    
    # 1. Get the model answers for each multihop question
    chat_response_generator.update_chat_history([
        ("system", "Answer the question with the name of an entity. Provide only the name of the entity as your answer. Please make an educated guess and always return an entity.\n\n[Here is one demonstration]\n\nUser:\nWho is the developer of Telegram?\n\nAssistant:\nTelegram FZ-LLC"),
    ])
    
    responses = chat_response_generator.generate_response(
        f"User:\n{item["question"]}\nAssistant:\n",
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        n=args.num_responses,
        max_tokens=args.max_tokens,
    )
    responses = [response.replace("Assistant:", "").strip() for response in responses]
        
    # 2. Update the chain with the model's answers
    item["probe_answers"] = responses
    
    return item, chat_response_generator.get_usage()


if __name__ == "__main__":
    # Parse command line arguments
    args = parse_args()
    
    # Load the probe dataset
    probe_dataset = ProbeDataset(f'data/{args.task_name}/test_{args.data_size}_depth_{args.depth}.pkl')
    
    # Load the API key from the configuration file
    with open(args.api_config_file, 'r') as f:
        api_config = json.load(f)
        args.api_key = api_config.get("api_key", None)
        args.ua = api_config.get("wikimedia", None).get("user_agent", None)
        if args.api_key is None:
            raise ValueError("API key not found in the configuration file.")
        if args.ua is None:
            raise ValueError("User agent not found in the configuration file.")
        
    # Process all chains
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    processed_data = []
    chat_response_generator = ChatResponseGenerator(model_name=translate_model_name(args.model_name), api_key=args.api_key)
    
    # Use tqdm to show progress
    with tqdm(total=len(probe_dataset), desc="Processing chains") as pbar:
        for i in range(len(probe_dataset)):
            # Process each item
            item = probe_dataset[i]
            processed_item, usage = probe_item(item, args, chat_response_generator)
            # Update the total token counts
            translated_model_name = translate_model_name(args.model_name)
            prompt_tokens += usage[translated_model_name]["prompt_tokens"]
            completion_tokens += usage[translated_model_name]["completion_tokens"]
            total_tokens += usage[translated_model_name]["total_tokens"]
            # Append the processed chain to the list
            processed_data.append(processed_item)
            # Update the progress bar with the number of tokens used
            pbar.set_postfix_str(f"Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}")
            pbar.update(1)
            # Save the processed data on the fly
            with open(f'data/{args.task_name}/probe_test_{args.data_size}_depth_{args.depth}_{args.model_name}.pkl', 'wb') as f:
                pickle.dump(processed_data, f)

    # Save the processed data to a new pickle file
    with open(f'data/eval_results/{args.task_name}/probe/test_{args.data_size}_depth_{args.depth}_{args.model_name}.pkl', 'wb') as f:
        pickle.dump(processed_data, f)
